{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fee90d8d-567c-42dd-a294-2f1678f42be4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create a [Mosaic AI Model Serving](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/model-serving/) endpoint to serve an [`External Model`](https://docs.databricks.com/en/generative-ai/external-models/index.html)  \n",
    "\n",
    "[`At the time of this Solution's original development, GPT-4o just surfaced. Today you have many other Foundational Models to consider. However, we use Azure GPT-4o here to illustrated how to create an external model serving endpoint.`]\n",
    "\n",
    "In order to leverage `GPT-4o's Medical and Scientific knowledge capabilities` (ref to `Health` and `Scientific Capabilities` sections under `Social Impacts` in [`gpt-4o-system-card`](https://openai.com/index/gpt-4o-system-card/)), we will serve the external `Azure Openai GPT-4o` model endpoint to use with our [AIBI Genie Space](https://docs.databricks.com/en/genie/index.html) \n",
    "\n",
    "(_Omit steps `1-3` if your workspace already has an **external** endpoint serving e.g. `gpt-4o` with which you can call and make inference_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "687376da-eb69-4dcb-b26d-a71a85b64628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "There are different ways to create external endpoints to serve `{Azure}OpenAI/non-Databricks` models e.g. via the [UI](https://docs.databricks.com/en/machine-learning/model-serving/create-foundation-model-endpoints.html#ext-model-endpoint) or[`mlflow.deployments`](https://docs.databricks.com/en/generative-ai/tutorials/external-models-tutorial.html) (_NB: at the time of documenting, `mlflow.deployments` does not support Mosaic AI Gateway options_).\n",
    "\n",
    "Here, we use the [serving-endpoints API](https://docs.databricks.com/en/machine-learning/model-serving/create-foundation-model-endpoints.html#language-REST%C2%A0API) ([ref](https://docs.databricks.com/api/workspace/servingendpoints/create)) to serve the `Azure OpenAI GPT4o` model and include the [Mosaic AI Gateway](https://docs.databricks.com/en/ai-gateway/index.html) options e.g. \n",
    "- track endpoint usage and associated costs using [system tables](https://docs.databricks.com/en/admin/system-tables/index.html) \n",
    "- include payload logging for model inference data audit using [inference table](https://docs.databricks.com/en/machine-learning/model-serving/inference-tables.html#what-is) which could be further used for monitoring if desired   \n",
    "\n",
    "[These options can be configured using the UI or specified via code](https://docs.databricks.com/en/ai-gateway/configure-ai-gateway-endpoints.html).    \n",
    "\n",
    "\n",
    "<!-- # ref \n",
    "# https://docs.databricks.com/en/generative-ai/tutorials/external-models-tutorial.html \n",
    "\n",
    "# https://docs.databricks.com/en/machine-learning/model-serving/create-foundation-model-endpoints.html#language-REST%C2%A0API\n",
    "# https://docs.databricks.com/api/workspace/servingendpoints/create\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f6d6827-5a0a-47bb-98c3-0e07235bf63c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [1] The following pre-requisites are needed: \n",
    "\n",
    "- [Azure OpenAI API subscription](https://portal.azure.com/#create/Microsoft.CognitiveServicesOpenAI) (Keys & Endpoint Info.)\n",
    "  - API Base = Endpoint URL: `https://<served_ai_foundry_endpoint_name>.openai.azure.com/` \n",
    "  <!-- - API Base = Endpoint URL: `https://hls-fedemo-azure-openai.openai.azure.com/`  -->\n",
    "  - API KEY: _to store as [`Databricks CLI`](https://docs.databricks.com/en/dev-tools/cli/index.html) [`secret` within a `scope`](https://docs.databricks.com/en/security/secrets/index.html)_\n",
    "\n",
    "- [Azure AI Foundry/Azure OpenAI Service/Deployments](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=web-portal#deploy-a-model) \n",
    "  - Deploy required external model\n",
    "    - Deployment Name (under Deployment Info.): `gpt-4o-2024-11-20` (NB model version = `2024-11-20`)\n",
    "    - Model API Version (from Endpoint Target URI): `2024-08-01-preview`\n",
    "\n",
    "    [_NB: at the time this solution was developed these versions were selected -- newer versions likely replaced these over time -- please choose a version that is currently available for your needs._ NOTE also that model version `YYYY-MM-DD` can differ from Model API Version `YYY-MM-DD-{description}`]\n",
    "\n",
    "- Workspace [`Personal Access Token`](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth/pat) OR [Azure Entra ID Token-based](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/service-prin-aad-token) [`Service Principal`](https://learn.microsoft.com/en-us/azure/databricks/admin/users-groups/service-principals) (recommended for production workloads) \n",
    "\n",
    "<!-- \n",
    "Internal Ref: https://docs.google.com/document/d/1Sj-Bz0vRNi6AGYxDBugkoD4Luscevq86Tqio8uLO9GE/edit?pli=1&tab=t.0 \n",
    "\n",
    "## AZ-OPENAI-subscription\n",
    "# https://portal.azure.com/#@DataBricksInc.onmicrosoft.com/resource/subscriptions/3f2e4d32-8e8d-46d6-82bc-5bb8d962328b/resourceGroups/fe-shared-amer-001/providers/Microsoft.CognitiveServices/accounts/hls-fedemo-azure-openai/overview\n",
    "\n",
    "# https://portal.azure.com/#@DataBricksInc.onmicrosoft.com/resource/subscriptions/3f2e4d32-8e8d-46d6-82bc-5bb8d962328b/resourceGroups/fe-shared-amer-001/providers/Microsoft.CognitiveServices/accounts/hls-fedemo-azure-openai/cskeys\n",
    "\n",
    "# KEY1: setup scope + secret \n",
    "\n",
    "# endpoint/base: https://hls-fedemo-azure-openai.openai.azure.com/\n",
    "\n",
    "## DEPLOYMENT\n",
    "# https://ai.azure.com/resource/deployments/%2Fsubscriptions%2F3f2e4d32-8e8d-46d6-82bc-5bb8d962328b%2FresourceGroups%2Ffe-shared-amer-001%2Fproviders%2FMicrosoft.CognitiveServices%2Faccounts%2Fhls-fedemo-azure-openai%2Fdeployments%2Fgpt-4o-2024-11-20?wsid=/subscriptions/3f2e4d32-8e8d-46d6-82bc-5bb8d962328b/resourceGroups/fe-shared-amer-001/providers/Microsoft.CognitiveServices/accounts/hls-fedemo-azure-openai&tid=9f37a392-f0ae-4280-9796-f1864a10effc\n",
    "\n",
    "# target uri: https://hls-fedemo-azure-openai.openai.azure.com/openai/deployments/gpt-4o-2024-11-20/chat/completions?api-version=2024-08-01-preview\n",
    "\n",
    "# base: https://hls-fedemo-azure-openai.openai.azure.com/\n",
    "# deployment name: gpt-4o-2024-11-20   ## model version 2024-11-20\n",
    "# api version: 2024-08-01-preview\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0929d48a-d826-4bf4-a3f7-de4c847f8e6c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Databricks CLI"
    }
   },
   "source": [
    "### [2] Create Databricks [Secret](https://docs.databricks.com/en/security/secrets/index.html) Scope + Secret for [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service) [API](https://portal.azure.com/#create/Microsoft.CognitiveServicesOpenAI) KEY\n",
    "\n",
    "The [`Databricks CLI`](https://docs.databricks.com/en/dev-tools/cli/index.html) is only supported for interactive use from the web terminal on x86 compute.    \n",
    "We recommend using    \n",
    "i) the [notebook web terminal to invoke CLI commands](https://learn.microsoft.com/en-us/azure/databricks/notebooks/notebook-ui#cli) to [create scopes and store secrets](https://docs.databricks.com/en/security/secrets/index.html#create-a-secret); OR     \n",
    "2) the [Databricks Python SDK](https://databricks-sdk-py.readthedocs.io/en/latest/workspace/workspace/secrets.html) if you would like to interface with Databricks APIs.\n",
    "\n",
    "\n",
    "We provide some guiding code for Scope and corresponding Secret setup using Databricks CLI here:   \n",
    "\n",
    "Check databricks path:\n",
    "```\n",
    "\n",
    ">> which databricks\n",
    "/usr/local/bin/databricks\n",
    " \n",
    "```   \n",
    "\n",
    "Check which databricks version ... which prompts the Databricks CLI installion on notebook terminal: \n",
    "```\n",
    "\n",
    ">> databricks --version\n",
    "Installing the CLI...\n",
    "Installed Databricks CLI v0.237.0 at /root/bin/databricks.\n",
    "Databricks CLI v0.237.0\n",
    " \n",
    "```    \n",
    "\n",
    "Once installed we can use the CLI to list existing Scopes: \n",
    "```\n",
    "\n",
    ">> databricks secrets list-scopes \n",
    " \n",
    "```    \n",
    "\n",
    "<!-- We will use `scope_name= hls_fedemo_azure_openai` to create a new `scope` and include a `secret` corresponding to the `scope_key= azopenai_api_key` :   -->\n",
    "We will use **`scope_name`** `= \"<{your_scope_name_prefix}_azure_openai>\"` to create a new `scope` and include a `secret` corresponding to the **`scope_key`** `= \"<azopenai_api_key>\"` : \n",
    "\n",
    "```\n",
    "\n",
    "databricks secrets create-scope {scope_name} \n",
    "\n",
    "databricks secrets put-secret {scope_name} {scope_key}\n",
    " \n",
    "```\n",
    "---     \n",
    "\n",
    "For our example: \n",
    "```\n",
    "\n",
    "databricks secrets create-scope {your_scope_name_prefix}-azure-openai\n",
    "\n",
    "databricks secrets put-secret {your_scope_name_prefix}-azure-openai openai_api_key   \n",
    "```  \n",
    "_(This prompts for `KEY` which we input with the `Azure OpenAI API KEY`)_\n",
    "\n",
    "\n",
    "We can check the defined scope and secret using `databricks secrets get-secret {scope}, {key}`.   \n",
    "(_NB: this outputs a pseudo version of the secret, however, the copy-pasted secret is stored_) \n",
    "```\n",
    "\n",
    "databricks secrets get-secret {your_scope_name_prefix}-azure-openai openai_api_key \n",
    " \n",
    "```\n",
    "\n",
    "\n",
    "Likewise, we can check the defined scope and secret using `dbuitls.secrets.get(scope, key)` -- it will be shown as `['REDACTED]'` \n",
    "\n",
    "```\n",
    " \n",
    "dbutils.secrets.get(\"{your_scope_name_prefix}-azure-openai\", \"openai_api_key\")\n",
    "\n",
    "'[REDACTED]'\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ea8aa8b-61fe-448f-b770-2d1942b70985",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Scope with Databricks SDK"
    }
   },
   "outputs": [],
   "source": [
    "## If you don't already have one created ...\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "scope_name= \"<{your_scope_name_prefix}_azure_openai>\"  ## update to use <ServicePrinciple> reference\n",
    "scope_key= \"<azopenai_api_key>\"\n",
    "\n",
    "w.secrets.create_scope(scope_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85625e09-8e5a-4f37-8794-bdee606300ac",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Scope Key with Secret"
    }
   },
   "outputs": [],
   "source": [
    "w.secrets.put_secret(scope_name,scope_key,string_value =\"<secret>\") ## do not leave secret exposed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2d6ac82-6548-48cd-b47f-825442c6cb0d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read/Get_secret"
    }
   },
   "outputs": [],
   "source": [
    "# w.secrets.get_secret(scope_name,scope_key).value\n",
    "# (NB: this outputs a pseudo version of the secret, however, the copy-pasted secret is stored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a9ad1ea-8a79-444f-ae6c-bfd7a4367fb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [3] Define & Serve the `Azure Openai GPT-4o` External Model as an Endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "464b3734-bcab-42d9-912a-f1b1de963f93",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "run notebook utils"
    }
   },
   "outputs": [],
   "source": [
    "%run ./utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fe3a963-db18-4f45-ab87-98d1e127bec3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "UC variables"
    }
   },
   "outputs": [],
   "source": [
    "remove_widgets() \n",
    "uc_config = setup_uc_paths(use_widgets=False, print_endpoint=True, silent=False); \n",
    "## if you use the widgets and update the values in widgets -- UC paths will be automatically updated\n",
    "\n",
    "# Extract catalog, schema, volume names\n",
    "catalog_name = uc_config[\"catalog_name\"]\n",
    "schema_name = uc_config[\"schema_name\"]\n",
    "volume_name = uc_config[\"volume_name\"]\n",
    "external_endpoint_name = uc_config[\"external_endpoint_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1956bed2-e9a0-4d00-8f97-7f1d4624934f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set the environment variables"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "## Set the environment variables\n",
    "os.environ[\"DATABRICKS_HOST\"] = \"https://{workspace-instance}.{cloud or shard}.databricks.com/\"\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = dbutils.secrets.get({scope_name}, {scope_key}) ## PAT token | Best Practice: Service Principal (SP) token is recommended for production; require privileges/permissions to create SP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0671bc3c-7cc9-4511-abe3-f928c59f3afd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Use serving-endpoints API"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Retrieve the Databricks host and token from environment variables\n",
    "databricks_host = os.getenv(\"DATABRICKS_HOST\")\n",
    "databricks_token = os.getenv(\"DATABRICKS_TOKEN\")\n",
    "\n",
    "# Check if the environment variables are set\n",
    "if not databricks_host or not databricks_token:\n",
    "    raise ValueError(\"Databricks host and token must be set in environment variables\")\n",
    "\n",
    "# Define the endpoint URL\n",
    "url = f\"{databricks_host}/api/2.0/serving-endpoints\"\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {databricks_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Define the payload\n",
    "payload = {\n",
    "    \"name\": \"<external_endpoint_name e.g. az_openai_gpt4o>\",\n",
    "    \"config\": {\n",
    "        \"served_entities\": [\n",
    "            {\n",
    "                \"name\": \"az-openai-completions\",\n",
    "\n",
    "                \"external_model\": {\n",
    "                    \"name\": \"gpt-4o\",\n",
    "                    \"provider\": \"openai\",\n",
    "                    \"task\": \"llm/v1/chat\",\n",
    "                    \"openai_config\": {\n",
    "                        \"openai_api_type\": \"azure\",\n",
    "                        \"openai_api_key\": f\"{{{{secrets/{scope_name}/{scope_key}}}}}\",\n",
    "                        \"openai_api_base\": \"https://<served_ai_foundry_endpoint_name>.openai.azure.com/\",\n",
    "                        # \"openai_api_base\": \"https://hls-fedemo-azure-openai.openai.azure.com/\",\n",
    "                        # \"openai_api_key\": \"{{secrets/hls_fedemo_azure_openai/azopenai_api_key}}\", #databricks cli/sdk registered \n",
    "                        \"openai_deployment_name\": \"gpt-4o-2024-11-20\", \n",
    "                        \"openai_api_version\": \"2025-01-01-preview\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    \"ai_gateway\": {\n",
    "      \"usage_tracking_config\": {\n",
    "        \"enabled\": True\n",
    "      },\n",
    "      \"inference_table_config\": {\n",
    "          \"catalog_name\": \"demos_genie\",\n",
    "          \"schema_name\": \"hls_ai_drug_discovery\",\n",
    "          \"table_name_prefix\": \"\", #\"your_table_prefix\",\n",
    "          \"enabled\": True\n",
    "      }\n",
    "    },\n",
    "      \"tags\": [\n",
    "          {\n",
    "              \"key\": \"removeAfter\",\n",
    "              \"value\": \"2026-01-31\"\n",
    "          },\n",
    "          {\n",
    "              \"key\": \"<project>\",\n",
    "              \"value\": \"<name_of_project e.g. ai-driven_drug_discovery>\"\n",
    "          },\n",
    "          {\n",
    "              \"key\": \"do-not-delete\",\n",
    "              \"value\": \"True\"\n",
    "          }\n",
    "      ]\n",
    "  }\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "# Check the response\n",
    "if response.status_code == 200:\n",
    "    print(\"Endpoint created successfully\")\n",
    "else:\n",
    "    print(f\"Failed to create endpoint: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09fe9357-4a40-44d7-8a55-be1b5f490537",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Deployed Serving endpoint"
    }
   },
   "source": [
    "#### Deployed serving endpoint Info.:\n",
    "- ws endpoint: [az_openai_gpt4o](https://e2-demo-west.cloud.databricks.com/ml/endpoints/az_openai_gpt4o?o=2556758628403379)\n",
    "- serving-endpoint: `https://e2-demo-west.cloud.databricks.com/serving-endpoints/az_openai_gpt4o/invocations`\n",
    "- [az_openai_gpt4o_payload](https://e2-demo-west.cloud.databricks.com/explore/data/demos_genie/hls_ai_drug_discovery/az_openai_gpt4o_payload?o=2556758628403379): `demos_genie.hls_ai_drug_discovery.az_openai_gpt4o_payload`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f56251de-e46f-4040-a0fb-b990844133ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [4] Test using the External Endpoint for inferencing via [`ai_query()`](https://docs.databricks.com/en/large-language-models/ai-functions.html#ai_query)\n",
    "\n",
    "Now that the endpoint has been successfully created, we can test making inferencing before using it in our AI/BI & Genie Space.       \n",
    "Here we will demonstrate batch inferencing using `ai_query()`.    \n",
    "\n",
    "Below is a template of how we setup this batch inference in SQL:\n",
    "\n",
    "```\n",
    "\n",
    "SELECT\n",
    "  {input_column},   -- Placeholder for the input column\n",
    "  ai_query(\n",
    "    'az_openai_gpt4o',\n",
    "    CONCAT({prompt_placeholder}, {input_column})    -- Placeholder for the prompt and input\n",
    "  ) AS {output_column}  -- Placeholder for the output column\n",
    "FROM {table_name};  -- Placeholder for the table name\n",
    "LIMIT 50\n",
    "```\n",
    "\n",
    "- Note: [Pay-per-token foundation models](https://docs.databricks.com/en/machine-learning/foundation-model-apis/index.html#pay-per-token-foundation-model-apis) are good for testing but have limits.    \n",
    "- They are unsuitable for processing more than 100 rows; use [provisioned foundation models](https://docs.databricks.com/en/machine-learning/foundation-model-apis/deploy-prov-throughput-foundation-model-apis.html) for larger datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "600ef6e0-b86e-4170-86c8-d407849e00fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd56b400-96aa-4f87-94b5-fb1d86938e5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "remove_widgets() \n",
    "uc_config = setup_uc_paths(use_widgets=False, print_endpoint=True, silent=True);\n",
    "\n",
    "# Extract catalog, schema, volume names\n",
    "catalog_name = uc_config[\"catalog_name\"]\n",
    "schema_name = uc_config[\"schema_name\"]\n",
    "volume_name = uc_config[\"volume_name\"]\n",
    "external_endpoint_name = uc_config[\"external_endpoint_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16ad6343-ab78-4567-b832-84401b365097",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761956125953}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "TEST external gpt4o endpoint"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "## Parameters \n",
    "# catalog_name = \"<catalog_name>\" #\"<your_catalog_name>\" \n",
    "# schema_name = \"ai_driven_drug_discovery\" #\"<your_schema_name>\"\n",
    "# external_endpoint_name = \"az_openai_gpt4o\" ## name of deployed external endpoint on your workspace\n",
    "\n",
    "score_threshold = 0.85\n",
    "organism_filter = \"%human%\"\n",
    "\n",
    "# Read tables\n",
    "protein_df = spark.table(f\"{catalog_name}.{schema_name}.proteinclassification_tiny\")\n",
    "organism_df = spark.table(f\"{catalog_name}.{schema_name}.tinysample_organism_info_scientificNsimple\")\n",
    "\n",
    "# Filter and join\n",
    "filtered_df = (\n",
    "    protein_df\n",
    "    .filter(F.col(\"score\") > score_threshold)\n",
    "    .join(\n",
    "        organism_df,\n",
    "        protein_df.OrganismName == organism_df.OrganismName,\n",
    "        \"inner\"\n",
    "    )\n",
    "    .filter(F.lower(F.col(\"Organism_SimpleTerm\")).like(organism_filter.lower()))\n",
    ")\n",
    "\n",
    "# Create AI query prompt\n",
    "prompt_template = (\n",
    "    \"You are well-versed in membrane proteins and drug discovery research. \"\n",
    "    \"Please be brief. Provide a dictionary of responses in the format \"\n",
    "    '{\"key\": \"response\"} to the following keys \"information\", \"recent_research\" '\n",
    "    'and highlight \"under_researched_areas\" that hold promise for drug discovery '\n",
    "    'for the given \"protein name\": {protein_name} '\n",
    "    'Output just the dictionary {\"information\": \"response\", \"recent_research\": \"response\", '\n",
    "    '\"under_researched_areas\": \"response\"}. Do not include \"```json\" strings in output'\n",
    ")\n",
    "\n",
    "# Build final dataframe with AI query\n",
    "result_df = (\n",
    "    filtered_df\n",
    "    .withColumn(\n",
    "        \"ai_prompt\",\n",
    "        F.concat(\n",
    "            F.lit(prompt_template.replace(\"{protein_name}\", \"\")),\n",
    "            F.col(\"ProteinName\")\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"researchDict\",\n",
    "        F.expr(f\"ai_query('{external_endpoint_name}', ai_prompt)\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"information\",\n",
    "        F.get_json_object(F.col(\"researchDict\"), \"$.information\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"recent_research\",\n",
    "        F.get_json_object(F.col(\"researchDict\"), \"$.recent_research\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"under_researched_areas\",\n",
    "        F.get_json_object(F.col(\"researchDict\"), \"$.under_researched_areas\")\n",
    "    )\n",
    "    .select(\n",
    "        organism_df.OrganismName,\n",
    "        F.col(\"Organism_SimpleTerm\"),\n",
    "        protein_df.ProteinName,\n",
    "        F.col(\"researchDict\"),\n",
    "        F.col(\"information\"),\n",
    "        F.col(\"recent_research\"),\n",
    "        F.col(\"under_researched_areas\"),\n",
    "        F.col(\"label\").alias(\"ProteinType\"),\n",
    "        F.col(\"score\").alias(\"ProteinClassificationScore\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display or save results\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d42095cc-0529-4194-b7c4-65d0c4ab99dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [5] Register the [`ai_query()`](https://docs.databricks.com/aws/en/sql/language-manual/functions/ai_query) as `SQL_function` to Unity Catalog \n",
    "Similarly to previous example, We can register the `ai_query()` calling the external Foundation Model `az_openai_gpt4o` to help with getting protein related research infomation associated with Organism of interest. This makes it easier for calling the registered SQL_function later in either AI/BI Dashboard/Genie Space or even in the Playground. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93b0e3de-f258-47c0-b6e3-82c0d46a2192",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Register as ai_query() as SQL function"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# AI_MODEL = \"<external_endpoint_name e.g. az_openai_gpt4o>\"\n",
    "AI_MODEL = external_endpoint_name\n",
    "FUNCTION_NAME = f\"{catalog_name}.{schema_name}.get_protein_research_info\"\n",
    "\n",
    "# System prompt (easier to maintain)\n",
    "SYSTEM_PROMPT = \"\"\"You are a membrane proteins and drug discovery expert.\n",
    "Analyze the given protein and return ONLY valid JSON with these exact keys:\n",
    "- information: brief protein overview\n",
    "- recent_research: key recent findings  \n",
    "- under_researched_areas: promising drug discovery opportunities\n",
    "\n",
    "Output format: {\"information\": \"...\", \"recent_research\": \"...\", \"under_researched_areas\": \"...\"}\n",
    "No markdown. No code blocks. Be concise.\"\"\"\n",
    "\n",
    "create_function_query = f\"\"\"\n",
    "CREATE OR REPLACE FUNCTION {FUNCTION_NAME}(ProteinName STRING)\n",
    "RETURNS STRUCT<information: STRING, recent_research: STRING, under_researched_areas: STRING>\n",
    "RETURN \n",
    "  FROM_JSON(\n",
    "    ai_query(\n",
    "      '{AI_MODEL}',\n",
    "      CONCAT('{SYSTEM_PROMPT}\\n\\nProtein: \"', REPLACE(ProteinName, '\"', ''), '\"')\n",
    "    ),\n",
    "    'STRUCT<information: STRING, recent_research: STRING, under_researched_areas: STRING>'\n",
    "  );\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_function_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9488f16c-0e18-4fb5-a665-51db8a4b2911",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test Bulk process with SQL_function"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "score_threshold = 0.8\n",
    "organism_filter = \"%Zebrafish%\"  # Options: \"%human%\", \"%Zebrafish%\", etc.\n",
    "output_table_name = \"tinysample_organism_protein_research_info\"\n",
    "save_results = False  # Set to True to save results\n",
    "\n",
    "# Build the query\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "  OrganismName,\n",
    "  Organism_SimpleTerm,\n",
    "  ProteinName,\n",
    "  researchDict.information AS information,\n",
    "  researchDict.recent_research AS recent_research,\n",
    "  researchDict.under_researched_areas AS under_researched_areas,\n",
    "  ProteinType,\n",
    "  ProteinClassificationScore\n",
    "FROM (\n",
    "  SELECT\n",
    "    p.ProteinName,\n",
    "    p.label AS ProteinType,\n",
    "    p.score AS ProteinClassificationScore,\n",
    "    p.GeneName,\n",
    "    p.Sequence,\n",
    "    p.Molecular_Weight,\n",
    "    p.OrganismName,\n",
    "    o.Organism_SimpleTerm,\n",
    "    {catalog_name}.{schema_name}.get_protein_research_info(p.ProteinName) AS researchDict\n",
    "  FROM\n",
    "    {catalog_name}.{schema_name}.proteinclassification_tiny p\n",
    "  JOIN \n",
    "    {catalog_name}.{schema_name}.tinysample_organism_info_scientificNsimple o \n",
    "    ON p.OrganismName = o.OrganismName\n",
    "  WHERE\n",
    "    p.score > {score_threshold}\n",
    "    AND o.Organism_SimpleTerm ILIKE '{organism_filter}'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Display the query\n",
    "print(\"Generated SQL Query:\")\n",
    "print(\"=\" * 80)\n",
    "print(query)\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nQuery Parameters:\")\n",
    "print(f\"  - Catalog: {catalog_name}\")\n",
    "print(f\"  - Schema: {schema_name}\")\n",
    "print(f\"  - Score Threshold: {score_threshold}\")\n",
    "print(f\"  - Organism Filter: {organism_filter}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Execute the query\n",
    "print(\"\\nExecuting query...\")\n",
    "sDF_protein_research_info = spark.sql(query)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nQuery executed successfully. Displaying results...\")\n",
    "display(sDF_protein_research_info)\n",
    "\n",
    "# Optional: Save results to table\n",
    "if save_results:\n",
    "    output_table = f\"{catalog_name}.{schema_name}.{output_table_name}\"\n",
    "    print(f\"\\nSaving results to table: {output_table}\")\n",
    "    sDF_protein_research_info.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(output_table)\n",
    "    print(f\"Results saved successfully to {output_table}\")\n",
    "else:\n",
    "    print(\"\\nNote: Results not saved. Set save_results=True to save to table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "803557ab-9def-48b0-948c-ab8fdcd1e23f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [6] Bulk process to derive `protein_research_info` \n",
    "- In order to more efficiently explore without waiting for the queries to run for specific `Organism_SimpleTerm` and `ProteinClassificationScore` during ad-hoc explorations, we will pre-process to extract the information pertaining to `recent research` and `\"under_researched_areas\" that hold promise for drug discovery` for the list of proteins in our `tinysample`. \n",
    "- Although this will take a bit of time up-front, it will allow for more efficient and simpler filtering of the inferred proteins and the research-relevant outputs. \n",
    "- For production use, [provisioned throughput model serving endpoint](https://docs.databricks.com/en/machine-learning/foundation-model-apis/deploy-prov-throughput-foundation-model-apis.html#provisioned-throughput-endpoint-ui) is highly recommended if [Databricks hosted Foundation Model APIs](https://docs.databricks.com/aws/en/machine-learning/model-serving/model-serving-limits#provisioned-throughput-limits) can be leveraged!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "870e2821-7bd1-4f40-887b-d0766cfb2cc0",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762156640099}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "pyspark Bulk process with SQL_function"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Parameters\n",
    "\n",
    "score_threshold = None  # Options: None (no filter), 0.8, 0.9, etc.\n",
    "organism_filter = None  # Options: None (all organisms), \"%human%\", \"%Zebrafish%\", etc.\n",
    "output_table_name = \"tinysample_organism_protein_research_info\"\n",
    "save_results = True  # Set to False to skip saving\n",
    "display_results = True  # Set to False to skip displaying results\n",
    "display_limit = 100  # Options: None (show all), 100, 500, etc.\n",
    "\n",
    "# Build the WHERE clause conditionally\n",
    "where_conditions = []\n",
    "\n",
    "if score_threshold is not None:\n",
    "    where_conditions.append(f\"p.score > {score_threshold}\")\n",
    "\n",
    "if organism_filter is not None:\n",
    "    where_conditions.append(f\"o.Organism_SimpleTerm ILIKE '{organism_filter}'\")\n",
    "\n",
    "# Only add WHERE clause if there are conditions\n",
    "where_clause = f\"WHERE {' AND '.join(where_conditions)}\" if where_conditions else \"\"\n",
    "\n",
    "# Build the base query\n",
    "base_query = f\"\"\"\n",
    "SELECT\n",
    "  p.ProteinName,\n",
    "  p.label AS ProteinType,\n",
    "  p.score AS ProteinClassificationScore,\n",
    "  p.GeneName,\n",
    "  p.Sequence,\n",
    "  p.Molecular_Weight,\n",
    "  p.OrganismName,\n",
    "  o.Organism_SimpleTerm\n",
    "FROM\n",
    "  {catalog_name}.{schema_name}.proteinclassification_tiny p\n",
    "JOIN \n",
    "  {catalog_name}.{schema_name}.tinysample_organism_info_scientificNsimple o \n",
    "  ON p.OrganismName = o.OrganismName\n",
    "{where_clause}\n",
    "\"\"\"\n",
    "\n",
    "# Display the query\n",
    "print(\"Generated Base SQL Query:\")\n",
    "print(\"=\" * 80)\n",
    "print(base_query)\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nQuery Parameters:\")\n",
    "print(f\"  - Catalog: {catalog_name}\")\n",
    "print(f\"  - Schema: {schema_name}\")\n",
    "print(f\"  - Score Threshold: {score_threshold if score_threshold is not None else 'None (no filter)'}\")\n",
    "print(f\"  - Organism Filter: {organism_filter if organism_filter else 'None (no filter)'}\")\n",
    "print(f\"  - Display Results: {display_results}\")\n",
    "print(f\"  - Display Limit: {display_limit if display_limit else 'None (show all)'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Execute base query\n",
    "print(\"\\nExecuting base query...\")\n",
    "df_base = spark.sql(base_query)\n",
    "print(f\"Base query executed. Row count: {df_base.count()}\")\n",
    "\n",
    "# Apply the get_protein_research_info function and extract STRUCT fields\n",
    "print(\"\\nApplying protein research info function and extracting STRUCT fields...\")\n",
    "df_research_info = (\n",
    "    df_base\n",
    "    .withColumn(\n",
    "        \"researchDict\", \n",
    "        F.expr(f\"{catalog_name}.{schema_name}.get_protein_research_info(ProteinName)\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"information\", \n",
    "        F.col(\"researchDict.information\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"recent_research\", \n",
    "        F.col(\"researchDict.recent_research\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"under_researched_areas\", \n",
    "        F.col(\"researchDict.under_researched_areas\")\n",
    "    )\n",
    "    .select(\n",
    "        \"OrganismName\",\n",
    "        \"Organism_SimpleTerm\",\n",
    "        \"ProteinName\",\n",
    "        \"researchDict\",\n",
    "        \"information\",\n",
    "        \"recent_research\",\n",
    "        \"under_researched_areas\",\n",
    "        \"ProteinType\",\n",
    "        \"ProteinClassificationScore\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display results (conditionally)\n",
    "if display_results:\n",
    "    print(\"\\nDisplaying research info results...\")\n",
    "    if display_limit is not None:\n",
    "        print(f\"(Limited to {display_limit} rows)\")\n",
    "        display(df_research_info.limit(display_limit))\n",
    "    else:\n",
    "        print(\"(Showing all rows)\")\n",
    "        display(df_research_info)\n",
    "else:\n",
    "    print(\"\\nNote: Display skipped. Set display_results=True to display results.\")\n",
    "\n",
    "# Save to Delta Table\n",
    "if save_results:\n",
    "    output_table = f\"{catalog_name}.{schema_name}.{output_table_name}\"\n",
    "    print(f\"\\nSaving results to table: {output_table}\")\n",
    "    \n",
    "    # Drop table first to avoid schema conflicts\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {output_table}\")\n",
    "    \n",
    "    # Save without mergeSchema option\n",
    "    df_research_info.write.mode(\"overwrite\").saveAsTable(output_table)\n",
    "    print(f\"Results saved successfully to {output_table}\")\n",
    "else:\n",
    "    print(\"\\nNote: Results not saved. Set save_results=True to save to table.\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXECUTION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total proteins processed: {df_research_info.count()}\")\n",
    "print(f\"Score threshold: {score_threshold if score_threshold is not None else 'None (no filter)'}\")\n",
    "print(f\"Organism filter applied: {organism_filter if organism_filter else 'None (no filter)'}\")\n",
    "print(f\"Results displayed: {display_results}\")\n",
    "if display_results and display_limit:\n",
    "    print(f\"Display limit: {display_limit} rows\")\n",
    "print(f\"Results saved: {save_results}\")\n",
    "if save_results:\n",
    "    print(f\"Output table: {catalog_name}.{schema_name}.{output_table_name}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "773a25e1-9299-49c5-a210-138e45a1dc35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check/Read tinysample_organism_protein_research_info"
    }
   },
   "outputs": [],
   "source": [
    "sDF_protein_research_info = spark.table(f\"{catalog_name}.{schema_name}.tinysample_organism_protein_research_info\")  \n",
    "\n",
    "display(sDF_protein_research_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ced5e83d-7a09-4aad-b385-f27808ffdcb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2588019255242724,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "5.0_Create&Query_ExternalEndpoint_gpt4o",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8824d638-8a12-443f-a3c5-498724a6b101",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install necessary packages."
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q biopython\n",
    "dbutils.library.restartPython()\n",
    "\n",
    "## For Spark Declarative Pipelines, dependecies need to be installed right at the top of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6928ff1b-63de-4db9-b877-f240d28fe178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Protein Data Pre-Processing using Spark Declarative Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a95db51-02e9-4780-9e55-df3daba41d23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For our [AI/BI Dashboard](https://docs.databricks.com/en/dashboards/index.html) with [enabled Genie Space](https://docs.databricks.com/en/dashboards/index.html#enable-a-genie-space-from-your-dashboard) example we will use the [UniProt FASTA protein sequence data](https://www.uniprot.org/help/downloads), which we downloaded to the [Unity Catalog](https://docs.databricks.com/en/catalogs/index.html) [Volumes](https://docs.databricks.com/en/volumes/index.html) associated with our project [Schema](https://docs.databricks.com/en/schemas/index.html#what-is-a-schema): `<catalog_name>.ai_driven_drug_discovery`\n",
    "As with most `raw` data, we will need to pre-process to 'clean' and extract relevant information before we can interact and use the data for our downstream explorations. \n",
    "\n",
    "Here we will define a `Protein Data Processing` [Spark Declarative Pipelines](https://www.databricks.com/product/delta-live-tables) to help extract the protein information e.g. *`identifiers`*, *`sequences`*, *`names`*, *`organism information`*, *`gene names`*, *`protein existence`*, and perform any additional transformation(s) e.g. calculating *`molecular weight`* in a sequence of tasks.   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97cc41b8-500c-4cb5-909a-673f1baf5b77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create an ETL pipeline via `Jobs & Pipelines`\n",
    "\n",
    "- Access via workspace LHS Tab\n",
    "- **Use this notebook as the `source` of the ETL pipeline**\n",
    "\n",
    "![](../assets/imgs/proteindata_ETL_with_SDP_combi.png)\n",
    "\n",
    "<!-- ![](../assets/imgs/Create_ETLpipeline.png) -->\n",
    "\n",
    "<!-- **Use this notebook as the `source` of the ETL pipeline** -->\n",
    "\n",
    "<!-- <br>  -->\n",
    "\n",
    "#### When defined and run, the resulting pipeline would include 3 notebook tasks as observed above\n",
    "\n",
    " <!-- <img src=\"../assets/imgs/ProteinDataProcessing_DLT.png\" alt=\"Protein Data Processing DLT\" /> -->\n",
    "\n",
    "The `protein_preprocessing.yaml` is provided for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "807d877a-ee26-4909-9706-cad9e3dc57b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d72ff73-dbe7-4e65-b582-3ec346d3824f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Specify UC paths for ELT pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b4d0000-e2e2-4e3c-8066-04370672d6b0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define UC variables"
    }
   },
   "outputs": [],
   "source": [
    "## REPLACE THE VALUES BELOW WITH YOUR OWN BEFORE configuring the ETL pipeline with this notebook\n",
    "\n",
    "catalog_name = \"<catalog_name>\"                ## Replace with <your_catalog_name>\n",
    "schema_name = \"ai_driven_drug_discovery\"       ## Replace with <your_schema_name>\n",
    "volume_name = \"protein_seq\"                    ## Replace with <your_volume_name> \n",
    "\n",
    "print(f\"Using catalog: {catalog_name}, schema: {schema_name}, volume: {volume_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e33cc8e5-3aad-4bd0-92c2-7fd49caa1d1c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define the DLT pipeline"
    }
   },
   "source": [
    "## Define Tasks for our ETL pipeline\n",
    "\n",
    "<!-- [Protein Data Processing](https://e2-demo-west.cloud.databricks.com/pipelines/c6a3e57b-1c44-476f-9e56-c8e05d9975f5/updates/2adf25f7-deb6-40fa-8f87-68efd0ca05a0?o=2556758628403379%3Fparent%3Dfolders%2F1625373258638091)  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0a304fd-cf47-44b1-8062-10404ac68ca3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import required packages/libraries."
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from Bio import SeqIO\n",
    "from pyspark.sql import Row\n",
    "import io\n",
    "\n",
    "# Additional library/package import will be included in cells related to the DLT pipeline Tasks specified below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae379519-b8cf-4298-ac2f-21877383b3fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [1] Create `bronze_protein`  \n",
    "Load 500,000 protein sequences from 1 text file into Bronze Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f8d8a08-e1b8-444d-a85d-6d711cb02a03",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read in FASTA file as DLT materialize table"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize lists to hold the data\n",
    "records = []\n",
    "\n",
    "# Read from UC Volumes and Parse the FASTA file\n",
    "\n",
    "file_path = f'/Volumes/{catalog_name}/{schema_name}/{volume_name}/uniprot_sprot.fasta'\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "     for record in SeqIO.parse(f, \"fasta\"):\n",
    "      id = record.id\n",
    "      sequence = str(record.seq)\n",
    "      description = record.description\n",
    "      \n",
    "      records.append(Row(ID=id, Sequence=sequence, Description=description))\n",
    "\n",
    "df = spark.createDataFrame(records)\n",
    "\n",
    "\n",
    "##Create a Bronze Delta Live Table\n",
    "@dlt.expect_or_drop(\"Empty ID\",\"ID != '' \")\n",
    "@dlt.table(\n",
    "    comment = \"FASTA Data\",\n",
    "    table_properties = {\"quality\": \"bronze\"})    \n",
    "def bronze_protein():\n",
    "    return spark.createDataFrame(records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7248012b-d8a7-4fbb-a5c0-288a27912710",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [2] Create `silver_protein`  \n",
    "Parse through the text and format data into columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77205beb-00c2-435e-bb98-29621f990229",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Extract Protein Info. from FASTA file"
    }
   },
   "outputs": [],
   "source": [
    "## Import require functions \n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "## Create a Silver Delta Live Table \n",
    "@dlt.expect_or_drop(\"Empty ID\",\"ID != '' \")\n",
    "@dlt.table\n",
    "def silver_protein():\n",
    "    # Regular expressions for each field\n",
    "    os_regex = r'OS=([^ ]+ [^ ]+|\\([^)]+\\))'\n",
    "    ox_regex = r'OX=(\\d+)'\n",
    "    gn_regex = r'GN=([^ ]+)'\n",
    "    pe_regex = r'PE=(\\d)'\n",
    "    sv_regex = r'SV=(\\d)'\n",
    "\n",
    "\n",
    "    fasta_df = dlt.read(\"bronze_protein\")\n",
    "\n",
    "    # Extract ProteinName\n",
    "    fasta_df = fasta_df.withColumn(\"ProteinName\", regexp_extract(\"Description\", r\" (.+?) OS=\", 1))\n",
    "\n",
    "    # Extract and create new columns for OrganismName, OrganismIdentifier, GeneName, ProteinExistence, SequenceVersion\n",
    "    fasta_df = fasta_df.withColumn('OrganismName', regexp_extract('Description', os_regex, 1))\n",
    "    fasta_df = fasta_df.withColumn('OrganismIdentifier', regexp_extract('Description', ox_regex, 1))\n",
    "    fasta_df = fasta_df.withColumn('GeneName', regexp_extract('Description', gn_regex, 1))\n",
    "    fasta_df = fasta_df.withColumn('ProteinExistence', regexp_extract('Description', pe_regex, 1))\n",
    "    fasta_df = fasta_df.withColumn('SequenceVersion', regexp_extract('Description', sv_regex, 1))\n",
    "\n",
    "    return fasta_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb0c3c6b-cb8e-4038-b60f-a4f514a2e3fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [3] Create `enriched_protein` \n",
    " We can also demonstrate how fast and easy it is to use third party libraries e.g. ```Bio.SeqUtils' molecular_weight``` within a [`Pandas` User-Defined Function](https://docs.databricks.com/en/udf/pandas.html) inside an ETL pipeline task to calculate ***molecular weights*** of each molecule in a vectorized process.    \n",
    "      \n",
    " Here, we are calculating *molecular weights* for 500,000 molecules and this takes about 30 seconds using a serverless DLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d330dc80-f80e-47f6-85b9-2f294fedaf2a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Include Molecular Weight Calculation"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from Bio.SeqUtils import molecular_weight\n",
    "from Bio.Seq import Seq\n",
    "import pandas as pd\n",
    "\n",
    "# Assume Arrow is already enabled in your Databricks cluster\n",
    "\n",
    "# Define our Pandas UDF to calculate molecular weight using Bio.SeqUtils' molecular_weight module\n",
    "@pandas_udf(DoubleType())\n",
    "def get_molecular_weight_pandas_udf(sequence: pd.Series) -> pd.Series:\n",
    "    def calculate_mw(seq):\n",
    "        try:\n",
    "            # Attempt to calculate the molecular weight\n",
    "            return molecular_weight(Seq(seq), seq_type=\"protein\")\n",
    "        except ValueError as e:\n",
    "            return 1.0\n",
    "                \n",
    "    \n",
    "    return sequence.apply(calculate_mw)\n",
    "\n",
    "# DLT Pipeline function to enrich and modify the table\n",
    "@dlt.expect_or_drop(\"Empty ID\",\"ID != '' \")\n",
    "@dlt.table\n",
    "def enriched_protein():\n",
    "    # Load the existing silver_protein_parsed table\n",
    "    df = dlt.read(\"silver_protein\")\n",
    "    \n",
    "    # Add the \"Molecular Weight\" column using the Pandas UDF to vectorize the molecular weights calculation\n",
    "    df = df.withColumn(\"Molecular_Weight\", get_molecular_weight_pandas_udf(df[\"sequence\"]))\n",
    "    \n",
    "    # Drop the \"Description\" column from the DataFrame\n",
    "    df = df.drop(\"Description\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "094dac23-dec3-4ebd-a3cf-c4a1618b6a28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7c5d01d-b65b-4bf2-acbc-43d18eafa815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [4] Review our defined ETL pipeline & `enriched_protein` sparkDF\n",
    "\n",
    "- Defined ETL pipeline: refer to associated pipeline job    \n",
    "\n",
    "- Use ``` (Ctrl + `)``` to view Terminal, ETL, Logs within notebook  \n",
    "- Turn on ETL Pipeline editor "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 913238801222378,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "2.0_ProteinData_ETL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
